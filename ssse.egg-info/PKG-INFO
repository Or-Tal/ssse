Metadata-Version: 2.1
Name: magma
Version: 0.0.0
Summary: Models for Automatic Generation of Audio and Music
Home-page: https://github.com/fairinternal/audiocraft
Author: FAIR Speech & Audio
License: MIT License
Classifier: License :: OSI Approved :: MIT License
Classifier: Topic :: Multimedia :: Sound/Audio
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.8.0
Description-Content-Type: text/markdown
Provides-Extra: dev


# ðŸŒ‹ MAGMA: Models for Automatic Generation of Music and Audio
![linter badge](https://github.com/fairinternal/audiocraft/workflows/magma_linter/badge.svg)
![tests badge](https://github.com/fairinternal/audiocraft/workflows/magma_tests/badge.svg)

MAGMA is our framework supporting our research on models for automatic generation of music and audio.

## Core principles

**Research friendly**: The Magma package contains our research framework designed to be research and experiment-friendly.
It is built on top of PyTorch as our core deep learning library, [Flashy](https://github.com/fairinternal/flashy) to build
distributed training pipelines, and [Audiocraft](https://github.com/fairinternal/audiocraft) for our audio components.
The goal of Magma is to remain flexible for fast iterations of new research ideas.

**Holy Audiocraft**: The Audiocraft package contains core components that are useful across research projects, with trustworthy
and tested implementation. Audiocraft is versioned and is expected to be **always stable** (all tests must pass at all time
on master, version and tags maintained).

## Getting started

### Install

Create a conda environment, and install dependencies:

```shell
# first install the latest (or desired) audiocraft version
pip install -e .
cd magma
# install magma-specific dependencies
pip install -e '.[dev]'
```

### Running tests and linter

```shell
# if working on audiocraft, make sure that audiocraft tests are passing as well
make
# magma specific tests and linting
cd magma
make               # run everything
make linter        # run linters
make tests         # run unit tests
```

### Launching jobs

Try launching jobs for different tasks locally with dora run:

```shell
cd magma
# run dummy model and solver
dora run solver=dummy/dummy_v1
# run compression task with lightweight encodec
dora run solver=compression/debug
```

To launch jobs on the cluster, use `dora launch ... -g <nb_gpu>`.

Most of the time, the jobs are launched through dora grids, for example:

```shell
cd magma
# run dummy grid
dora grid dummy.dummy
# run compression task with RVQ quantization
dora grid compression.encodec_rvq_24k
```

Learn more about using dora in the [dedicated section](#a-short-intro-to-dora).

## Available tasks and solvers

### Overview

| Solver | Description | Stable |
|---|---|---|
| [`DummySolver`](./magma/solvers/dummy.py) | Debug task (audio reconstruction with L1) | ðŸŸ¢ |
| [`CompressionSolver`](./magma/solvers/compression.py) | Compression with perceptual loss (EnCodec) | ðŸŸ¢ |
| [`LMSolver`](./magma/solvers/lm.py) | Language modeling on top of compression codes | ðŸŸ  |

#### Overriding parameters

As we'll cover below, the different MAGMA solvers inherit the same base implementation, hence
we can use the same parameters to control the number of samples used for each stage,
the batch size, etc. We can override each parameters directly or use a config group to override a
set of parameters (e.g. `dset=<dataset_name>`, `model=<model_name>`).

```shell
# let's run some task with the default params for the solver
dora run solver=dummy/dummy_v1
# now let's say we want to change the dataset on which we run our experiments
dora run solver=dummy/dummy_v1 dset=jamendo_24
# now let's say we want to speed training because we are just debugging
dora run solver=dummy/dummy_v1 optim.epochs=10 optim.updates_per_epochs=100
# we may also want to reduce the batch size or number of samples considered for a stage
dora run solver=dummy/dummy_v1 dataset.batch_size=8 dataset.train.num_samples=1000 dataset.valid.num_samples=1000
# we may also want to change valid frequency (e.g. every 10 epochs) or checkpoint frequency (e.g. every 20 epochs)
dora run solver=dummy/dummy_v1 valid.every=10 checkpoint.save_every=20
# or restart from a given checkpoint or sig
dora run solver=dummy/dummy_v1 checkpoint.continue_from=fa8717f9
```

Note that we can also launch the jobs on the cluster using `dora launch` or through a grid
that will contain the parameter sweep we would like to experiment with. Please check
the [section on Dora](#a-short-intro-to-dora) to learn more.

### Dummy task

The [`DummySolver`](./magma/solvers/dummy.py) implements a minimalistic solver training
a very simple encoder-decoder model for audio reconstruction using a single loss (e.g. L1).
We can use this task to test our MAGMA setup and make sure we can run MAGMA tasks with dora.

```shell
cd magma

# run dummy model and solver
dora run solver=dummy/dummy_v1
dora run solver=dummy/dummy_v1 device=cpu # CPU-only training and inference

# run dummy grid
dora grid dummy.dummy
```

### Compression task

The [`CompressionSolver`](./magma/solvers/compression.py) implements the audio reconstruction
task to train an EnCodec-like model. It trains an encoder-decoder with a quantization bottleneck -
a SEANet encoder-decoder with Residual Vector Quantization bottleneck for EnCodec - using a combination
of objective and perceptual losses in the forms of discriminators, relying on a loss balancer
to effectively weight the different losses in an intuitive manner.

The default configuration matches a causal EnCodec training with at a single bandwidth.

```shell
dora run solver=compression/encodec_rvq_24k
# run with a different dset
dora run solver=compression/encodec_rvq_24k dset=audio/mmi_24k

# example grid
dora grid compression.encodec_rvq_causal_mmi24
```

#### Evaluation

Evaluations metrics for audio generation:
* SI-SNR: Scale-Invariant Signal-to-Noise Ratio ([Luo & Mesgarani, 2019](https://arxiv.org/pdf/1809.07454.pdf)).
* ViSQOL: [Virtual Speech Quality Objective Listener](https://github.com/google/visqol).

Note: Path to the ViSQOL binary (compiled with bazel) needs to be provided in order to run
the ViSQOL metric on the reference and degraded signals. The metric is disabled by default.

```shell
# enabling ViSQOL at evaluation time
dora run solver=compression/encodec_rvq_24k evaluate.metrics.visqol=true metrics.visqol.bin=<PATH_TO_VISQOL_BIN>
```

#### Generation

The generation stage consists in generating the reconstructed audio from samples with the current model.
The number of samples generated and the batch size used are controlled by the `dataset.generate` configuration.
The output path and audio formats are defined in the `generate` stage configuration.

```shell
# generate samples every 5 epoch
dora run solver=compression/encodec_rvq_24k generate.every=5
# run with a different dset
dora run solver=compression/encodec_rvq_24k generate.path=<PATH_IN_DORA_XP_FOLDER>
# limit the number of samples or use a different batch size
dora grid solver=compression/encodec_rvq_24k dataset.generate.num_samples=10 dataset.generate.batch_size=4
```

### Language modeling task

The [`LMSolver`](./magma/solvers/lm.py) implements causal language modeling on top of multi-stream
discrete codes. For now, the discrete codes are extracted on-the-fly from a pre-trained EnCodec model
(see `CompressionSolver` to learn how to train such model).

For now, we support the following strategies for multi-stream modeling:
* Parallel prediction of the different streams

#### Evaluation

Both valid and evaluate stages are performed in a streaming fashion. We return both the cross entropy
and the perplexity of the model on the current dataset split.

#### Generation

The generation stage allows to generate samples unconditionnally or conditional to an audio prompt (for now).
We currently support greedy sampling (argmax), sampling from softmax with a given temperature, top-K and top-P
(nucleus) sampling. The number of samples generated and the batch size used are controlled by the `dataset.generate` configuration while the other different generation parameters are defined in `generate.lm`.

```shell
# control sampling parameters
dora run solver=lm/debug generate.lm.max_gen_len=512 generate.lm.use_sampling=true generate.lm.top_k=15
# generate 5 unconditional samples
dora run solver=lm/debug generate.lm.unconditional_samples=5
```

## MAGMA codebase

### Main components

#### Solver objects and the standard solver

MAGMAâ€™s core component is the solver, which is the definition of how to solve a given task. It implements the training pipeline logic,
combining the datasets, model, optimization criterion and components and the full training loop. We refer the reader to
[Flashy](https://github.com/fairinternal/flashy) for core principles around solvers.

MAGMA proposes an initial solver StandardSolver that can be used as the base implementation for downstream solvers. This standard solver
provides a nice base management of logging, checkpoints loading/saving, xp restoration, etc. Most of MAGMA tasks are likely to follow
the same set of stages: `train`, `valid`, `evaluate` and `generate`, hence the solvers expect the splits of data named as these stages.

Each solver is responsible for defining the task to solve and the associated stages of the training loop in order to leave the full ownership
of the training pipeline to the researchers. This includes loading the datasets, building the model and optimisation components, registering
them and defining the execution of each stage. One can further customise its own solver starting from scratch instead of inheriting from
the standard solver.

To create a new solver for a given task, one should extend the [StandardSolver](./magma/solvers/base.py) and define each stage of the training loop.

```python

from . import base
from .. import optim


class MyNewSolver(base.StandardSolver):

    def __init__(self, cfg: omegaconf.DictConfig):
        super().__init__(cfg)
        # one can add custom attributes to the solver
        self.criterion = torch.nn.L1Loss()

    def build_model(self):
        # here you can instantiate your models and optimization related objects
        # this method will be called by the StandardSolver init method
        self.model = ...
        # the self.cfg attribute contains the raw configuration
        self.optimizer = optim.build_optimizer(self.model.parameters(), self.cfg.solver.optim)
        # don't forget to register the states you'd like to include in your checkpoints!
        self.register_stateful('model', 'optimizer')

    def build_dataloaders(self):
        # here you can instantiate your dataloaders
        # this method will be called by the StandardSolver init method
        self.dataloaders = ...

    # then you are expected to define the different stages: train, valid, evaluate and generate.
    # the standard solver provides a very simple loop over the epochs, executing one stage after the other
    # following the conditions specified in the configuration on every which epoch a stage should run.
    def train(self):
        self.logger.info("Implement train!")

    def valid(self):
        self.logger.info("Implement valid!")

    def evaluate(self):
        self.logger.info("Implement evaluate!")

    def generate(self):
        self.logger.info("Implement generate!")
```

Examples:
* A very simple example of a task can be found in the [DummySolver](./magma/solvers/dummy.py).
* For a more advanced example, check out the [CompressionSolver](./magma/solvers/compression.py).

#### Model objects

In MAGMA, a model is a container object that wraps one or more torch modules together with potential processing logic to use in a solver.
For example, a model would wrap an encoder module, a quantisation bottleneck module, a decoder and some tensor processing logic.
Each of the previous components can be considered as a small Â« model unit Â» on its own but the container model is a practical component
to manipulate and train a set of modules together.

#### Configuration

MAGMA's configuration is defined in yaml files and the framework relies on [hydra](https://hydra.cc/docs/intro/) and
[omegaconf](https://omegaconf.readthedocs.io/) to parse and manipulate the configuration through Dora.

##### :warning: Important considerations around configurations

Our configuration management relies on Hydra and the concept of group configs to structure and compose configurations.
Updating the root default configuration files will then have an impact on all solvers and tasks.
**One should never change the default configuration files. Instead they should use Hydra config groups in order to store custom configuration.**
Once this configuration is created and used for running experiments, you should not edit it anymore.

Note that as we are using Dora as our experiment manager, all our experiment tracking is based on signatures computed from
delta between configurations. **One must therefore ensure backward compatibilty of the configuration at all time.**
See [Dora's README](https://github.com/facebookresearch/dora) and the [section below introduction Dora](#a-short-intro-to-dora).

##### Configuration structure

The configuration is organized in config groups:
* `adversary`: default values for adversaries and adversarial training.
* `dset`: contains all data source related information (paths to manifest files and metadata for a given dataset).
* `loss`: default values for losses.
* `model`: contains configuration for each model defined in MAGMA and configurations for different variants of models.
* `solver`: contains the default configuration for each solver as well as configuration for each solver task, combining all the above components.

The `config.yaml` file is the main configuration that composes the above groups and contains default configuration for MAGMA.

##### Solver's core configuration structure

The core configuration structure shared across solver is available in `solvers/solver_default.yaml`.

#### Other modules

The magma codebase also contains all required implementations for our research in the other modules and you are welcome
to contribute to it as well, adding the different components required for your exploration.

#### Datasets

We store our dataset manifest files in the `egs` folder. Each dataset is organized in a dedicated folder (eg. `mydataset`)
with nested folders for each of the split. The manifest metadata are stored as 1-JSON-per-line files and by convention the manifest is
stored in each split folder as `data.jsonl` or `data.jsonl.gz`.

We commit our manifest files so that they are shared across all researchers. As the manifest files can become very
large and that we will end up with many datasets, we rely on [git LFS](https://git-lfs.github.com/) to store
the datasets appropriately.

A dedicated datasource is created for each dataset in the configuration to be then used in MAGMA, containing the
pointers to the paths of the manifest files for each split along with additional information (eg. maximum sample rate
to use against this dataset). These data sources configuration are gathered in the `dset` group, with a dedicated yaml
file for each dataset.

##### Using a specific dataset

You can specify the dataset to use relying on the dset group, eg:

```shell
dset=<dataset_name> # <dataset_name> should match the yaml file name
```

##### Adding a new dataset

Here are the steps to add a new dataset to MAGMA:
1. Generate the manifest JSON files for each split in a dedicated directory under `egs`, eg. `egs/mydataset`.
2. Create a new yaml file in `config/dset` with the name of the dataset (eg. `dset/mydataset.yaml`) and specify
the manifest paths and metadata.
3. Test using your dataset for your task.
4. Add the created manifest files to git using [git LFS](https://git-lfs.github.com/), add the yaml config
as a regular file, commit and push.

**Using git LFS for egs files**

For instance, let's assume that following the above steps, you created a dataset named `mydataset`
with the following structure:

```shell
egs/
  mydataset/
    train/data.jsonl.gz
    valid/data.jsonl.gz
    evaluate/data.jsonl.gz
    generate/data.jsonl.gz
```

Here is how to commit all the files using git LFS:

```shell
# select the file types you'd like Git LFS to manage
git lfs track "egs/mydataset/*/*.jsonl.gz"
# now make sure .gitattributes is tracked
git add .gitattributes
# add the files that are tracked
git add egs/mydataset
# don't forget to also add the config file for the new dataset
git add config/dset/mydataset.yaml
# then commit and push to GitHub as you normally would;
# for instance, if your current branch is named mybranch
git commit -m "Add mydataset dataset"
git push origin mybranch
```

## Contribution guidelines

### Development workflow

Please work in a branch until you have a stable code to ship in the main branch. Submit a PR to be reviewed before merging to main.
**At all time, the linter and tests must be successful on the main branch and on PR to this branch for both audiocraft and for magma.**

### Supporting new tasks

Create a new solver for any new task you'd like to add. When it makes sense, a solver may support multiple types of models. We expect the solver
for a task to have a base configuration defined in a nested group in the `solver` group config, and variants / extensions of this to be defined
as well in this folder.

Follow the example above on how to extend the `StandardSolver` to support a new task. We expect you to add the required unit tests and integration
tests when contributing to MAGMA with new tasks. A good practice is to add at least one lightweight integration test containing your new solver
in the `Makefile` when submitting your PR.

#### :warning: On working together

As explained above, updating the root default configuration files will then have an impact on all solvers and tasks.
**One should never change the default configuration files (including defaults for a given solver). Instead they should use Hydra config groups in order to store custom configuration.**

Please remind that experiment tracking is based on signatures computed from delta between configurations,
**one must therefore ensure backward compatibilty of the configuration and code at all time.**

### Parameter sweeps with Dora grid

We expect each task to contain a dedicated module in `magma/grids` to contain all the grid definitions associated to this solver.
If you want to start from an existing grid, please copy it in a new file to avoid corrupting others work.

### Adding new datasets

Check out the section on adding new datasets above.

## A short intro to Dora

[Dora](https://github.com/facebookresearch/dora) is the experiment manager tool used in MAGMA. Check out the README to learn how
Dora works. Here is a quick summary of what to know:
* An XP is a unique set of hyper-parameters with a given signature. The signature is a hash of those hyper-parameters.
We always refer to an XP with its signature, e.g. 9357e12e. We will see after that one can retrieve the hyper-params
and re-rerun it in a single command.
* In fact, the hash is defined as a delta between the base config and the one obtained with the config overrides you passed from the command line.
This means you must never change the conf/**.yaml files directly., except for editing things like paths. Changing the default values in the config
files means the XP signature won't reflect that change, and wrong checkpoints might be reused. I know, this is annoying, but the reason is that
otherwise, any change to the config file would mean that all XPs ran so far would see their signature change.

### Dora commands

```shell
dora info -f 81de367c  # this will show the hyper-parameter used by a specific XP.
                       # Be careful some overrides might present twice, and the right most one
                       # will give you the right value for it.
dora run -d -f 81de367c   # run an XP with the hyper-parameters from XP 81de367c.
                          # `-d` is for distributed, it will use all available GPUs.
dora run -d -f 81de367c dataset.batch_size=32  # start from the config of XP 81de367c but change some hyper-params.
                                               # This will give you a new XP with a new signature (here 3fe9c332).
```

An XP runs from a specific folder based on its signature, under the `<cluster_specific_path>/<user>/experiments/magma/outputs/` folder.
You can safely interrupt a training and resume it, it will reuse any existing checkpoint, as it will reuse the same folder.
If you made some change to the code and need to ignore a previous checkpoint you can use `dora run --clear [RUN ARGS]`.

If you have a Slurm cluster, you can also use the dora grid command, e.g.

```shell
# run dummy grid
dora grid dummy.dummy
```

Please refer to the Dora documentation for more information.
